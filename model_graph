digraph {
	graph [size="99.6,99.6"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140397787137744 [label="
 (1, 2, 128, 160)" fillcolor=darkolivegreen1]
	140397786798192 [label=ConvolutionBackward0]
	140397786798528 -> 140397786798192
	140397786798528 [label=ReluBackward0]
	140397786799056 -> 140397786798528
	140397786799056 [label=NativeBatchNormBackward0]
	140397786798816 -> 140397786799056
	140397786798816 [label=ConvolutionBackward0]
	140397786799008 -> 140397786798816
	140397786799008 [label=ReluBackward0]
	140397786799296 -> 140397786799008
	140397786799296 [label=NativeBatchNormBackward0]
	140397786799920 -> 140397786799296
	140397786799920 [label=ConvolutionBackward0]
	140397786799728 -> 140397786799920
	140397786799728 [label=CatBackward0]
	140397786800256 -> 140397786799728
	140397786800256 [label=ConvolutionBackward0]
	140397786800112 -> 140397786800256
	140397786800112 [label=ReluBackward0]
	140397786800736 -> 140397786800112
	140397786800736 [label=NativeBatchNormBackward0]
	140397786800832 -> 140397786800736
	140397786800832 [label=ConvolutionBackward0]
	140397786801120 -> 140397786800832
	140397786801120 [label=ReluBackward0]
	140397786801360 -> 140397786801120
	140397786801360 [label=NativeBatchNormBackward0]
	140397786801456 -> 140397786801360
	140397786801456 [label=ConvolutionBackward0]
	140397786801648 -> 140397786801456
	140397786801648 [label=CatBackward0]
	140397786801840 -> 140397786801648
	140397786801840 [label=ConvolutionBackward0]
	140397786801984 -> 140397786801840
	140397786801984 [label=ReluBackward0]
	140397786802176 -> 140397786801984
	140397786802176 [label=NativeBatchNormBackward0]
	140397786802272 -> 140397786802176
	140397786802272 [label=ConvolutionBackward0]
	140397786802464 -> 140397786802272
	140397786802464 [label=MulBackward0]
	140397786802656 -> 140397786802464
	140397786802656 [label=ReluBackward0]
	140397786802752 -> 140397786802656
	140397786802752 [label=NativeBatchNormBackward0]
	140397786802848 -> 140397786802752
	140397786802848 [label=ConvolutionBackward0]
	140397786803040 -> 140397786802848
	140397786803040 [label=MulBackward0]
	140397786803232 -> 140397786803040
	140397786803232 [label=CatBackward0]
	140397786803328 -> 140397786803232
	140397786803328 [label=ConvolutionBackward0]
	140397786803472 -> 140397786803328
	140397786803472 [label=ReluBackward0]
	140397786803664 -> 140397786803472
	140397786803664 [label=AddBackward0]
	140397786803760 -> 140397786803664
	140397786803760 [label=NativeBatchNormBackward0]
	140397786803904 -> 140397786803760
	140397786803904 [label=ConvolutionBackward0]
	140397786804096 -> 140397786803904
	140397786804096 [label=ReluBackward0]
	140397786804288 -> 140397786804096
	140397786804288 [label=NativeBatchNormBackward0]
	140397786804384 -> 140397786804288
	140397786804384 [label=MulBackward0]
	140397786804576 -> 140397786804384
	140397786804576 [label=ConvolutionBackward0]
	140397786803712 -> 140397786804576
	140397786803712 [label=ReluBackward0]
	140397786804816 -> 140397786803712
	140397786804816 [label=AddBackward0]
	140397786804912 -> 140397786804816
	140397786804912 [label=NativeBatchNormBackward0]
	140397786805056 -> 140397786804912
	140397786805056 [label=ConvolutionBackward0]
	140397786805248 -> 140397786805056
	140397786805248 [label=ReluBackward0]
	140397786805440 -> 140397786805248
	140397786805440 [label=NativeBatchNormBackward0]
	140397786805536 -> 140397786805440
	140397786805536 [label=MulBackward0]
	140397786805728 -> 140397786805536
	140397786805728 [label=ConvolutionBackward0]
	140397786803280 -> 140397786805728
	140397786803280 [label=ReluBackward0]
	140397786805968 -> 140397786803280
	140397786805968 [label=AddBackward0]
	140397786806064 -> 140397786805968
	140397786806064 [label=NativeBatchNormBackward0]
	140397786806208 -> 140397786806064
	140397786806208 [label=ConvolutionBackward0]
	140397786806400 -> 140397786806208
	140397786806400 [label=ReluBackward0]
	140397786806592 -> 140397786806400
	140397786806592 [label=NativeBatchNormBackward0]
	140397786806688 -> 140397786806592
	140397786806688 [label=MulBackward0]
	140397786806880 -> 140397786806688
	140397786806880 [label=ConvolutionBackward0]
	140397786806016 -> 140397786806880
	140397786806016 [label=ReluBackward0]
	140397786807120 -> 140397786806016
	140397786807120 [label=AddBackward0]
	140397786807216 -> 140397786807120
	140397786807216 [label=NativeBatchNormBackward0]
	140397786807360 -> 140397786807216
	140397786807360 [label=ConvolutionBackward0]
	140397786807552 -> 140397786807360
	140397786807552 [label=ReluBackward0]
	140397786807744 -> 140397786807552
	140397786807744 [label=NativeBatchNormBackward0]
	140397786807840 -> 140397786807744
	140397786807840 [label=MulBackward0]
	140397786808032 -> 140397786807840
	140397786808032 [label=ConvolutionBackward0]
	140397786801792 -> 140397786808032
	140397786801792 [label=ReluBackward0]
	140397786808272 -> 140397786801792
	140397786808272 [label=AddBackward0]
	140397786808176 -> 140397786808272
	140397786808176 [label=NativeBatchNormBackward0]
	140397778043136 -> 140397786808176
	140397778043136 [label=ConvolutionBackward0]
	140397778043328 -> 140397778043136
	140397778043328 [label=ReluBackward0]
	140397778043520 -> 140397778043328
	140397778043520 [label=NativeBatchNormBackward0]
	140397778043616 -> 140397778043520
	140397778043616 [label=ConvolutionBackward0]
	140397778042992 -> 140397778043616
	140397778042992 [label=ReluBackward0]
	140397778043952 -> 140397778042992
	140397778043952 [label=AddBackward0]
	140397778044048 -> 140397778043952
	140397778044048 [label=NativeBatchNormBackward0]
	140397778044192 -> 140397778044048
	140397778044192 [label=ConvolutionBackward0]
	140397778044384 -> 140397778044192
	140397778044384 [label=ReluBackward0]
	140397778044576 -> 140397778044384
	140397778044576 [label=NativeBatchNormBackward0]
	140397778044672 -> 140397778044576
	140397778044672 [label=ConvolutionBackward0]
	140397786800304 -> 140397778044672
	140397786800304 [label=ReluBackward0]
	140397778045008 -> 140397786800304
	140397778045008 [label=AddBackward0]
	140397778045104 -> 140397778045008
	140397778045104 [label=NativeBatchNormBackward0]
	140397778045248 -> 140397778045104
	140397778045248 [label=ConvolutionBackward0]
	140397778045440 -> 140397778045248
	140397778045440 [label=ReluBackward0]
	140397778045632 -> 140397778045440
	140397778045632 [label=NativeBatchNormBackward0]
	140397778045728 -> 140397778045632
	140397778045728 [label=ConvolutionBackward0]
	140397778045056 -> 140397778045728
	140397778045056 [label=ReluBackward0]
	140397778046064 -> 140397778045056
	140397778046064 [label=AddBackward0]
	140397778046160 -> 140397778046064
	140397778046160 [label=NativeBatchNormBackward0]
	140397778046304 -> 140397778046160
	140397778046304 [label=ConvolutionBackward0]
	140397778046496 -> 140397778046304
	140397778046496 [label=ReluBackward0]
	140397778046688 -> 140397778046496
	140397778046688 [label=NativeBatchNormBackward0]
	140397778046784 -> 140397778046688
	140397778046784 [label=ConvolutionBackward0]
	140397778046976 -> 140397778046784
	140397778046976 [label=ReluBackward0]
	140397778047168 -> 140397778046976
	140397778047168 [label=NativeBatchNormBackward0]
	140397778047264 -> 140397778047168
	140397778047264 [label=MulBackward0]
	140397778047456 -> 140397778047264
	140397778047456 [label=ConvolutionBackward0]
	140397778047552 -> 140397778047456
	140397786592848 [label="encoder.stem.convs.0.conv.weight
 (16, 3, 3, 3)" fillcolor=lightblue]
	140397786592848 -> 140397778047552
	140397778047552 [label=AccumulateGrad]
	140397778047504 -> 140397778047456
	140397786593088 [label="encoder.stem.convs.0.conv.bias
 (16)" fillcolor=lightblue]
	140397786593088 -> 140397778047504
	140397778047504 [label=AccumulateGrad]
	140397778047216 -> 140397778047168
	140397791187152 [label="encoder.stem.convs.0.norm.weight
 (16)" fillcolor=lightblue]
	140397791187152 -> 140397778047216
	140397778047216 [label=AccumulateGrad]
	140397778047072 -> 140397778047168
	140397797520256 [label="encoder.stem.convs.0.norm.bias
 (16)" fillcolor=lightblue]
	140397797520256 -> 140397778047072
	140397778047072 [label=AccumulateGrad]
	140397778046928 -> 140397778046784
	140397791186192 [label="encoder.stages.0.blocks.0.conv1.conv.weight
 (32, 16, 3, 3)" fillcolor=lightblue]
	140397791186192 -> 140397778046928
	140397778046928 [label=AccumulateGrad]
	140397778046880 -> 140397778046784
	140397791247504 [label="encoder.stages.0.blocks.0.conv1.conv.bias
 (32)" fillcolor=lightblue]
	140397791247504 -> 140397778046880
	140397778046880 [label=AccumulateGrad]
	140397778046736 -> 140397778046688
	140397791191792 [label="encoder.stages.0.blocks.0.conv1.norm.weight
 (32)" fillcolor=lightblue]
	140397791191792 -> 140397778046736
	140397778046736 [label=AccumulateGrad]
	140397778046592 -> 140397778046688
	140397786594608 [label="encoder.stages.0.blocks.0.conv1.norm.bias
 (32)" fillcolor=lightblue]
	140397786594608 -> 140397778046592
	140397778046592 [label=AccumulateGrad]
	140397778046448 -> 140397778046304
	140397786595168 [label="encoder.stages.0.blocks.0.conv2.conv.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	140397786595168 -> 140397778046448
	140397778046448 [label=AccumulateGrad]
	140397778046400 -> 140397778046304
	140397786595248 [label="encoder.stages.0.blocks.0.conv2.conv.bias
 (32)" fillcolor=lightblue]
	140397786595248 -> 140397778046400
	140397778046400 [label=AccumulateGrad]
	140397778046256 -> 140397778046160
	140397786595088 [label="encoder.stages.0.blocks.0.conv2.norm.weight
 (32)" fillcolor=lightblue]
	140397786595088 -> 140397778046256
	140397778046256 [label=AccumulateGrad]
	140397778046208 -> 140397778046160
	140397786726464 [label="encoder.stages.0.blocks.0.conv2.norm.bias
 (32)" fillcolor=lightblue]
	140397786726464 -> 140397778046208
	140397778046208 [label=AccumulateGrad]
	140397778046112 -> 140397778046064
	140397778046112 [label=NativeBatchNormBackward0]
	140397778046640 -> 140397778046112
	140397778046640 [label=ConvolutionBackward0]
	140397778046976 -> 140397778046640
	140397778047312 -> 140397778046640
	140397786727104 [label="encoder.stages.0.blocks.0.skip.0.conv.weight
 (32, 16, 1, 1)" fillcolor=lightblue]
	140397786727104 -> 140397778047312
	140397778047312 [label=AccumulateGrad]
	140397778046544 -> 140397778046112
	140397786727024 [label="encoder.stages.0.blocks.0.skip.0.norm.weight
 (32)" fillcolor=lightblue]
	140397786727024 -> 140397778046544
	140397778046544 [label=AccumulateGrad]
	140397778046352 -> 140397778046112
	140397786727184 [label="encoder.stages.0.blocks.0.skip.0.norm.bias
 (32)" fillcolor=lightblue]
	140397786727184 -> 140397778046352
	140397778046352 [label=AccumulateGrad]
	140397778045920 -> 140397778045728
	140397786727744 [label="encoder.stages.0.blocks.1.conv1.conv.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	140397786727744 -> 140397778045920
	140397778045920 [label=AccumulateGrad]
	140397778045872 -> 140397778045728
	140397786727824 [label="encoder.stages.0.blocks.1.conv1.conv.bias
 (32)" fillcolor=lightblue]
	140397786727824 -> 140397778045872
	140397778045872 [label=AccumulateGrad]
	140397778045680 -> 140397778045632
	140397786727664 [label="encoder.stages.0.blocks.1.conv1.norm.weight
 (32)" fillcolor=lightblue]
	140397786727664 -> 140397778045680
	140397778045680 [label=AccumulateGrad]
	140397778045536 -> 140397778045632
	140397786727904 [label="encoder.stages.0.blocks.1.conv1.norm.bias
 (32)" fillcolor=lightblue]
	140397786727904 -> 140397778045536
	140397778045536 [label=AccumulateGrad]
	140397778045392 -> 140397778045248
	140397786728464 [label="encoder.stages.0.blocks.1.conv2.conv.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	140397786728464 -> 140397778045392
	140397778045392 [label=AccumulateGrad]
	140397778045344 -> 140397778045248
	140397786728544 [label="encoder.stages.0.blocks.1.conv2.conv.bias
 (32)" fillcolor=lightblue]
	140397786728544 -> 140397778045344
	140397778045344 [label=AccumulateGrad]
	140397778045200 -> 140397778045104
	140397786728384 [label="encoder.stages.0.blocks.1.conv2.norm.weight
 (32)" fillcolor=lightblue]
	140397786728384 -> 140397778045200
	140397778045200 [label=AccumulateGrad]
	140397778045152 -> 140397778045104
	140397786728624 [label="encoder.stages.0.blocks.1.conv2.norm.bias
 (32)" fillcolor=lightblue]
	140397786728624 -> 140397778045152
	140397778045152 [label=AccumulateGrad]
	140397778045056 -> 140397778045008
	140397778044864 -> 140397778044672
	140397786729184 [label="encoder.stages.1.blocks.0.conv1.conv.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
	140397786729184 -> 140397778044864
	140397778044864 [label=AccumulateGrad]
	140397778044816 -> 140397778044672
	140397786729264 [label="encoder.stages.1.blocks.0.conv1.conv.bias
 (64)" fillcolor=lightblue]
	140397786729264 -> 140397778044816
	140397778044816 [label=AccumulateGrad]
	140397778044624 -> 140397778044576
	140397786729104 [label="encoder.stages.1.blocks.0.conv1.norm.weight
 (64)" fillcolor=lightblue]
	140397786729104 -> 140397778044624
	140397778044624 [label=AccumulateGrad]
	140397778044480 -> 140397778044576
	140397786729344 [label="encoder.stages.1.blocks.0.conv1.norm.bias
 (64)" fillcolor=lightblue]
	140397786729344 -> 140397778044480
	140397778044480 [label=AccumulateGrad]
	140397778044336 -> 140397778044192
	140397786729904 [label="encoder.stages.1.blocks.0.conv2.conv.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140397786729904 -> 140397778044336
	140397778044336 [label=AccumulateGrad]
	140397778044288 -> 140397778044192
	140397786729984 [label="encoder.stages.1.blocks.0.conv2.conv.bias
 (64)" fillcolor=lightblue]
	140397786729984 -> 140397778044288
	140397778044288 [label=AccumulateGrad]
	140397778044144 -> 140397778044048
	140397786729824 [label="encoder.stages.1.blocks.0.conv2.norm.weight
 (64)" fillcolor=lightblue]
	140397786729824 -> 140397778044144
	140397778044144 [label=AccumulateGrad]
	140397778044096 -> 140397778044048
	140397786730064 [label="encoder.stages.1.blocks.0.conv2.norm.bias
 (64)" fillcolor=lightblue]
	140397786730064 -> 140397778044096
	140397778044096 [label=AccumulateGrad]
	140397778044000 -> 140397778043952
	140397778044000 [label=NativeBatchNormBackward0]
	140397778044528 -> 140397778044000
	140397778044528 [label=ConvolutionBackward0]
	140397778044912 -> 140397778044528
	140397778044912 [label=AvgPool2DBackward0]
	140397786800304 -> 140397778044912
	140397778044960 -> 140397778044528
	140397786730624 [label="encoder.stages.1.blocks.0.skip.1.conv.weight
 (64, 32, 1, 1)" fillcolor=lightblue]
	140397786730624 -> 140397778044960
	140397778044960 [label=AccumulateGrad]
	140397778044432 -> 140397778044000
	140397786730544 [label="encoder.stages.1.blocks.0.skip.1.norm.weight
 (64)" fillcolor=lightblue]
	140397786730544 -> 140397778044432
	140397778044432 [label=AccumulateGrad]
	140397778044240 -> 140397778044000
	140397786730704 [label="encoder.stages.1.blocks.0.skip.1.norm.bias
 (64)" fillcolor=lightblue]
	140397786730704 -> 140397778044240
	140397778044240 [label=AccumulateGrad]
	140397778043808 -> 140397778043616
	140397786731264 [label="encoder.stages.1.blocks.1.conv1.conv.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140397786731264 -> 140397778043808
	140397778043808 [label=AccumulateGrad]
	140397778043760 -> 140397778043616
	140397786731344 [label="encoder.stages.1.blocks.1.conv1.conv.bias
 (64)" fillcolor=lightblue]
	140397786731344 -> 140397778043760
	140397778043760 [label=AccumulateGrad]
	140397778043568 -> 140397778043520
	140397786731184 [label="encoder.stages.1.blocks.1.conv1.norm.weight
 (64)" fillcolor=lightblue]
	140397786731184 -> 140397778043568
	140397778043568 [label=AccumulateGrad]
	140397778043424 -> 140397778043520
	140397786731424 [label="encoder.stages.1.blocks.1.conv1.norm.bias
 (64)" fillcolor=lightblue]
	140397786731424 -> 140397778043424
	140397778043424 [label=AccumulateGrad]
	140397778043280 -> 140397778043136
	140397786731984 [label="encoder.stages.1.blocks.1.conv2.conv.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140397786731984 -> 140397778043280
	140397778043280 [label=AccumulateGrad]
	140397778043232 -> 140397778043136
	140397786732064 [label="encoder.stages.1.blocks.1.conv2.conv.bias
 (64)" fillcolor=lightblue]
	140397786732064 -> 140397778043232
	140397778043232 [label=AccumulateGrad]
	140397778043088 -> 140397786808176
	140397786731904 [label="encoder.stages.1.blocks.1.conv2.norm.weight
 (64)" fillcolor=lightblue]
	140397786731904 -> 140397778043088
	140397778043088 [label=AccumulateGrad]
	140397778043040 -> 140397786808176
	140397786732144 [label="encoder.stages.1.blocks.1.conv2.norm.bias
 (64)" fillcolor=lightblue]
	140397786732144 -> 140397778043040
	140397778043040 [label=AccumulateGrad]
	140397778042992 -> 140397786808272
	140397786808128 -> 140397786808032
	140397786732704 [label="encoder.stages.2.blocks.0.conv1.conv.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	140397786732704 -> 140397786808128
	140397786808128 [label=AccumulateGrad]
	140397786808080 -> 140397786808032
	140397786732784 [label="encoder.stages.2.blocks.0.conv1.conv.bias
 (128)" fillcolor=lightblue]
	140397786732784 -> 140397786808080
	140397786808080 [label=AccumulateGrad]
	140397786807792 -> 140397786807744
	140397786732624 [label="encoder.stages.2.blocks.0.conv1.norm.weight
 (128)" fillcolor=lightblue]
	140397786732624 -> 140397786807792
	140397786807792 [label=AccumulateGrad]
	140397786807648 -> 140397786807744
	140397786732864 [label="encoder.stages.2.blocks.0.conv1.norm.bias
 (128)" fillcolor=lightblue]
	140397786732864 -> 140397786807648
	140397786807648 [label=AccumulateGrad]
	140397786807504 -> 140397786807360
	140397786733424 [label="encoder.stages.2.blocks.0.conv2.conv.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140397786733424 -> 140397786807504
	140397786807504 [label=AccumulateGrad]
	140397786807456 -> 140397786807360
	140397786733504 [label="encoder.stages.2.blocks.0.conv2.conv.bias
 (128)" fillcolor=lightblue]
	140397786733504 -> 140397786807456
	140397786807456 [label=AccumulateGrad]
	140397786807312 -> 140397786807216
	140397786733344 [label="encoder.stages.2.blocks.0.conv2.norm.weight
 (128)" fillcolor=lightblue]
	140397786733344 -> 140397786807312
	140397786807312 [label=AccumulateGrad]
	140397786807264 -> 140397786807216
	140397786733584 [label="encoder.stages.2.blocks.0.conv2.norm.bias
 (128)" fillcolor=lightblue]
	140397786733584 -> 140397786807264
	140397786807264 [label=AccumulateGrad]
	140397786807168 -> 140397786807120
	140397786807168 [label=NativeBatchNormBackward0]
	140397786807696 -> 140397786807168
	140397786807696 [label=ConvolutionBackward0]
	140397786808224 -> 140397786807696
	140397786808224 [label=AvgPool2DBackward0]
	140397786801792 -> 140397786808224
	140397786807936 -> 140397786807696
	140397786733984 [label="encoder.stages.2.blocks.0.skip.1.conv.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	140397786733984 -> 140397786807936
	140397786807936 [label=AccumulateGrad]
	140397786807600 -> 140397786807168
	140397786733904 [label="encoder.stages.2.blocks.0.skip.1.norm.weight
 (128)" fillcolor=lightblue]
	140397786733904 -> 140397786807600
	140397786807600 [label=AccumulateGrad]
	140397786807408 -> 140397786807168
	140397786734064 [label="encoder.stages.2.blocks.0.skip.1.norm.bias
 (128)" fillcolor=lightblue]
	140397786734064 -> 140397786807408
	140397786807408 [label=AccumulateGrad]
	140397786806976 -> 140397786806880
	140397786734624 [label="encoder.stages.2.blocks.1.conv1.conv.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140397786734624 -> 140397786806976
	140397786806976 [label=AccumulateGrad]
	140397786806928 -> 140397786806880
	140397786734704 [label="encoder.stages.2.blocks.1.conv1.conv.bias
 (128)" fillcolor=lightblue]
	140397786734704 -> 140397786806928
	140397786806928 [label=AccumulateGrad]
	140397786806640 -> 140397786806592
	140397786734544 [label="encoder.stages.2.blocks.1.conv1.norm.weight
 (128)" fillcolor=lightblue]
	140397786734544 -> 140397786806640
	140397786806640 [label=AccumulateGrad]
	140397786806496 -> 140397786806592
	140397786734784 [label="encoder.stages.2.blocks.1.conv1.norm.bias
 (128)" fillcolor=lightblue]
	140397786734784 -> 140397786806496
	140397786806496 [label=AccumulateGrad]
	140397786806352 -> 140397786806208
	140397786735264 [label="encoder.stages.2.blocks.1.conv2.conv.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140397786735264 -> 140397786806352
	140397786806352 [label=AccumulateGrad]
	140397786806304 -> 140397786806208
	140397786735344 [label="encoder.stages.2.blocks.1.conv2.conv.bias
 (128)" fillcolor=lightblue]
	140397786735344 -> 140397786806304
	140397786806304 [label=AccumulateGrad]
	140397786806160 -> 140397786806064
	140397786735184 [label="encoder.stages.2.blocks.1.conv2.norm.weight
 (128)" fillcolor=lightblue]
	140397786735184 -> 140397786806160
	140397786806160 [label=AccumulateGrad]
	140397786806112 -> 140397786806064
	140397786735424 [label="encoder.stages.2.blocks.1.conv2.norm.bias
 (128)" fillcolor=lightblue]
	140397786735424 -> 140397786806112
	140397786806112 [label=AccumulateGrad]
	140397786806016 -> 140397786805968
	140397786805824 -> 140397786805728
	140397786735984 [label="encoder.stages.3.blocks.0.conv1.conv.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	140397786735984 -> 140397786805824
	140397786805824 [label=AccumulateGrad]
	140397786805776 -> 140397786805728
	140397786736064 [label="encoder.stages.3.blocks.0.conv1.conv.bias
 (256)" fillcolor=lightblue]
	140397786736064 -> 140397786805776
	140397786805776 [label=AccumulateGrad]
	140397786805488 -> 140397786805440
	140397786735904 [label="encoder.stages.3.blocks.0.conv1.norm.weight
 (256)" fillcolor=lightblue]
	140397786735904 -> 140397786805488
	140397786805488 [label=AccumulateGrad]
	140397786805344 -> 140397786805440
	140397786736144 [label="encoder.stages.3.blocks.0.conv1.norm.bias
 (256)" fillcolor=lightblue]
	140397786736144 -> 140397786805344
	140397786805344 [label=AccumulateGrad]
	140397786805200 -> 140397786805056
	140397786736704 [label="encoder.stages.3.blocks.0.conv2.conv.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140397786736704 -> 140397786805200
	140397786805200 [label=AccumulateGrad]
	140397786805152 -> 140397786805056
	140397786736784 [label="encoder.stages.3.blocks.0.conv2.conv.bias
 (256)" fillcolor=lightblue]
	140397786736784 -> 140397786805152
	140397786805152 [label=AccumulateGrad]
	140397786805008 -> 140397786804912
	140397786736624 [label="encoder.stages.3.blocks.0.conv2.norm.weight
 (256)" fillcolor=lightblue]
	140397786736624 -> 140397786805008
	140397786805008 [label=AccumulateGrad]
	140397786804960 -> 140397786804912
	140397786737024 [label="encoder.stages.3.blocks.0.conv2.norm.bias
 (256)" fillcolor=lightblue]
	140397786737024 -> 140397786804960
	140397786804960 [label=AccumulateGrad]
	140397786804864 -> 140397786804816
	140397786804864 [label=NativeBatchNormBackward0]
	140397786805392 -> 140397786804864
	140397786805392 [label=ConvolutionBackward0]
	140397786805920 -> 140397786805392
	140397786805920 [label=AvgPool2DBackward0]
	140397786803280 -> 140397786805920
	140397786805632 -> 140397786805392
	140397786737424 [label="encoder.stages.3.blocks.0.skip.1.conv.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	140397786737424 -> 140397786805632
	140397786805632 [label=AccumulateGrad]
	140397786805296 -> 140397786804864
	140397786737344 [label="encoder.stages.3.blocks.0.skip.1.norm.weight
 (256)" fillcolor=lightblue]
	140397786737344 -> 140397786805296
	140397786805296 [label=AccumulateGrad]
	140397786805104 -> 140397786804864
	140397786737504 [label="encoder.stages.3.blocks.0.skip.1.norm.bias
 (256)" fillcolor=lightblue]
	140397786737504 -> 140397786805104
	140397786805104 [label=AccumulateGrad]
	140397786804672 -> 140397786804576
	140397786737904 [label="encoder.stages.3.blocks.1.conv1.conv.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140397786737904 -> 140397786804672
	140397786804672 [label=AccumulateGrad]
	140397786804624 -> 140397786804576
	140397786737984 [label="encoder.stages.3.blocks.1.conv1.conv.bias
 (256)" fillcolor=lightblue]
	140397786737984 -> 140397786804624
	140397786804624 [label=AccumulateGrad]
	140397786804336 -> 140397786804288
	140397786737824 [label="encoder.stages.3.blocks.1.conv1.norm.weight
 (256)" fillcolor=lightblue]
	140397786737824 -> 140397786804336
	140397786804336 [label=AccumulateGrad]
	140397786804192 -> 140397786804288
	140397786738064 [label="encoder.stages.3.blocks.1.conv1.norm.bias
 (256)" fillcolor=lightblue]
	140397786738064 -> 140397786804192
	140397786804192 [label=AccumulateGrad]
	140397786804048 -> 140397786803904
	140397786738544 [label="encoder.stages.3.blocks.1.conv2.conv.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140397786738544 -> 140397786804048
	140397786804048 [label=AccumulateGrad]
	140397786804000 -> 140397786803904
	140397786738624 [label="encoder.stages.3.blocks.1.conv2.conv.bias
 (256)" fillcolor=lightblue]
	140397786738624 -> 140397786804000
	140397786804000 [label=AccumulateGrad]
	140397786803856 -> 140397786803760
	140397786738464 [label="encoder.stages.3.blocks.1.conv2.norm.weight
 (256)" fillcolor=lightblue]
	140397786738464 -> 140397786803856
	140397786803856 [label=AccumulateGrad]
	140397786803808 -> 140397786803760
	140397786738784 [label="encoder.stages.3.blocks.1.conv2.norm.bias
 (256)" fillcolor=lightblue]
	140397786738784 -> 140397786803808
	140397786803808 [label=AccumulateGrad]
	140397786803712 -> 140397786803664
	140397786803424 -> 140397786803328
	140397789455248 [label="decoder.transpconvs.0.weight
 (256, 128, 2, 2)" fillcolor=lightblue]
	140397789455248 -> 140397786803424
	140397786803424 [label=AccumulateGrad]
	140397786803376 -> 140397786803328
	140397791103472 [label="decoder.transpconvs.0.bias
 (128)" fillcolor=lightblue]
	140397791103472 -> 140397786803376
	140397786803376 [label=AccumulateGrad]
	140397786803280 -> 140397786803232
	140397786802992 -> 140397786802848
	140397791187312 [label="decoder.stages.0.convs.0.conv.weight
 (128, 256, 3, 3)" fillcolor=lightblue]
	140397791187312 -> 140397786802992
	140397786802992 [label=AccumulateGrad]
	140397786802944 -> 140397786802848
	140397791183392 [label="decoder.stages.0.convs.0.conv.bias
 (128)" fillcolor=lightblue]
	140397791183392 -> 140397786802944
	140397786802944 [label=AccumulateGrad]
	140397786802800 -> 140397786802752
	140397791195872 [label="decoder.stages.0.convs.0.norm.weight
 (128)" fillcolor=lightblue]
	140397791195872 -> 140397786802800
	140397786802800 [label=AccumulateGrad]
	140397786802560 -> 140397786802752
	140397796208416 [label="decoder.stages.0.convs.0.norm.bias
 (128)" fillcolor=lightblue]
	140397796208416 -> 140397786802560
	140397786802560 [label=AccumulateGrad]
	140397786802416 -> 140397786802272
	140397786739424 [label="decoder.stages.0.convs.1.conv.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140397786739424 -> 140397786802416
	140397786802416 [label=AccumulateGrad]
	140397786802368 -> 140397786802272
	140397786739504 [label="decoder.stages.0.convs.1.conv.bias
 (128)" fillcolor=lightblue]
	140397786739504 -> 140397786802368
	140397786802368 [label=AccumulateGrad]
	140397786802224 -> 140397786802176
	140397786739344 [label="decoder.stages.0.convs.1.norm.weight
 (128)" fillcolor=lightblue]
	140397786739344 -> 140397786802224
	140397786802224 [label=AccumulateGrad]
	140397786802080 -> 140397786802176
	140397786739584 [label="decoder.stages.0.convs.1.norm.bias
 (128)" fillcolor=lightblue]
	140397786739584 -> 140397786802080
	140397786802080 [label=AccumulateGrad]
	140397786801936 -> 140397786801840
	140397786740224 [label="decoder.transpconvs.1.weight
 (128, 64, 2, 2)" fillcolor=lightblue]
	140397786740224 -> 140397786801936
	140397786801936 [label=AccumulateGrad]
	140397786801888 -> 140397786801840
	140397786740304 [label="decoder.transpconvs.1.bias
 (64)" fillcolor=lightblue]
	140397786740304 -> 140397786801888
	140397786801888 [label=AccumulateGrad]
	140397786801792 -> 140397786801648
	140397786801600 -> 140397786801456
	140397786740544 [label="decoder.stages.1.convs.0.conv.weight
 (64, 128, 3, 3)" fillcolor=lightblue]
	140397786740544 -> 140397786801600
	140397786801600 [label=AccumulateGrad]
	140397786801552 -> 140397786801456
	140397786740624 [label="decoder.stages.1.convs.0.conv.bias
 (64)" fillcolor=lightblue]
	140397786740624 -> 140397786801552
	140397786801552 [label=AccumulateGrad]
	140397786801408 -> 140397786801360
	140397786740464 [label="decoder.stages.1.convs.0.norm.weight
 (64)" fillcolor=lightblue]
	140397786740464 -> 140397786801408
	140397786801408 [label=AccumulateGrad]
	140397786801216 -> 140397786801360
	140397786740704 [label="decoder.stages.1.convs.0.norm.bias
 (64)" fillcolor=lightblue]
	140397786740704 -> 140397786801216
	140397786801216 [label=AccumulateGrad]
	140397786801072 -> 140397786800832
	140397786741264 [label="decoder.stages.1.convs.1.conv.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140397786741264 -> 140397786801072
	140397786801072 [label=AccumulateGrad]
	140397786801024 -> 140397786800832
	140397786741344 [label="decoder.stages.1.convs.1.conv.bias
 (64)" fillcolor=lightblue]
	140397786741344 -> 140397786801024
	140397786801024 [label=AccumulateGrad]
	140397786800784 -> 140397786800736
	140397786741184 [label="decoder.stages.1.convs.1.norm.weight
 (64)" fillcolor=lightblue]
	140397786741184 -> 140397786800784
	140397786800784 [label=AccumulateGrad]
	140397786800640 -> 140397786800736
	140397786741424 [label="decoder.stages.1.convs.1.norm.bias
 (64)" fillcolor=lightblue]
	140397786741424 -> 140397786800640
	140397786800640 [label=AccumulateGrad]
	140397786800160 -> 140397786800256
	140397786742144 [label="decoder.transpconvs.2.weight
 (64, 32, 2, 2)" fillcolor=lightblue]
	140397786742144 -> 140397786800160
	140397786800160 [label=AccumulateGrad]
	140397786800208 -> 140397786800256
	140397786742224 [label="decoder.transpconvs.2.bias
 (32)" fillcolor=lightblue]
	140397786742224 -> 140397786800208
	140397786800208 [label=AccumulateGrad]
	140397786800304 -> 140397786799728
	140397786799776 -> 140397786799920
	140397786742464 [label="decoder.stages.2.convs.0.conv.weight
 (32, 64, 3, 3)" fillcolor=lightblue]
	140397786742464 -> 140397786799776
	140397786799776 [label=AccumulateGrad]
	140397786799824 -> 140397786799920
	140397786742544 [label="decoder.stages.2.convs.0.conv.bias
 (32)" fillcolor=lightblue]
	140397786742544 -> 140397786799824
	140397786799824 [label=AccumulateGrad]
	140397786799248 -> 140397786799296
	140397786742384 [label="decoder.stages.2.convs.0.norm.weight
 (32)" fillcolor=lightblue]
	140397786742384 -> 140397786799248
	140397786799248 [label=AccumulateGrad]
	140397786799392 -> 140397786799296
	140397786742624 [label="decoder.stages.2.convs.0.norm.bias
 (32)" fillcolor=lightblue]
	140397786742624 -> 140397786799392
	140397786799392 [label=AccumulateGrad]
	140397786798960 -> 140397786798816
	140397787136464 [label="decoder.stages.2.convs.1.conv.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	140397787136464 -> 140397786798960
	140397786798960 [label=AccumulateGrad]
	140397786798912 -> 140397786798816
	140397787136544 [label="decoder.stages.2.convs.1.conv.bias
 (32)" fillcolor=lightblue]
	140397787136544 -> 140397786798912
	140397786798912 [label=AccumulateGrad]
	140397786799488 -> 140397786799056
	140397787136384 [label="decoder.stages.2.convs.1.norm.weight
 (32)" fillcolor=lightblue]
	140397787136384 -> 140397786799488
	140397786799488 [label=AccumulateGrad]
	140397786798384 -> 140397786799056
	140397787136624 [label="decoder.stages.2.convs.1.norm.bias
 (32)" fillcolor=lightblue]
	140397787136624 -> 140397786798384
	140397786798384 [label=AccumulateGrad]
	140397786798432 -> 140397786798192
	140397787137104 [label="decoder.seg_layers.2.weight
 (2, 32, 1, 1)" fillcolor=lightblue]
	140397787137104 -> 140397786798432
	140397786798432 [label=AccumulateGrad]
	140397786798480 -> 140397786798192
	140397787137184 [label="decoder.seg_layers.2.bias
 (2)" fillcolor=lightblue]
	140397787137184 -> 140397786798480
	140397786798480 [label=AccumulateGrad]
	140397786798192 -> 140397787137744
}
